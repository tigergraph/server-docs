= How to Replace a Node in a Cluster
:description: This page describes the procedure to replace a node in a non-ha cluster.

//welcome and introduction
This guide outlines the procedure for replacing a node in a cluster regardless of whether it is an High Availability(HA) cluster. If your system uses xref:ha-overview.adoc[High Availability], refer to the documentation on removing a failed node in xref:tigergraph-server:cluster-and-ha-management:remove-failed-node.adoc[Removal of Failed Nodes].

== Prerequisites
* TigerGraph is installed with HostName, refer to xref:bare-metal-install.adoc[HostName Installation]. Additionally, TigerGraph should be installed on a device that can be unmounted from the exiting machine and mounted to the new machine.
* This procedure applies to TigerGraph versions 3.10.1 and later. For versions prior to TigerGraph 3.10.1, please refer to link:https://docs.tigergraph.com/tigergraph-server/3.9/cluster-and-ha-management/how_to-replace-a-node-in-a-cluster[node replacement in preceding versions].
* The procedure requires pointing hostname to different IP addresses by changing DNS record. On AWS, this is done by link:https://docs.aws.amazon.com/route53/[Route 53]. For other cloud service providers, it is doable as long as they offer similar DNS web services.

== Procedure
//steps
. Create a Linux User on New Nodes:
+
Create a Linux user with the same username/password as your TigerGraph cluster.
+
[console, gsql]
----
sudo useradd tigergraph
sudo passwd tigergraph
----
+
. Prepare Files on New Nodes:
+
Prepare the following files not in the TigerGraph directory. These can be copied from any live nodes.
+
[console, gsql]
----
~/.ssh/
~/.tg.cfg
~/.bashrc
/etc/security/limits.d/98-tigergraph.conf
----
. Stop All Local Services:
+
Stop all local services using the command:
+
[console, gsql]
----
gadmin stop all --local
----
Use `gadmin stop all --local --ignore-error` if the node fails
. Unmount the device on which TigerGraph is installed with `umount` and the machine can be shut down.
. Mount the device to the new machine with the same mount point.
. Point the hostname of the exiting machine to the IP address of the new machine by updating the record in the DNS web service provided by your cloud service provider. For instance: 
+
On AWS, assuming the exiting node's hostname is `ip-172-31-26-200.us-west-2.compute.internal`. Initially the DNS server should resolve to its original IP address:
+
----
nslookup ip-172-31-26-200.us-west-2.compute.internal
...
Address: 172.31.26.200
----
Assuming the new IP machin'es IP address is `172.31.23.44`. After link:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-editing.html[updating the record on AWS Route 53] -- Note that you'll need to link:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html[create a private hosted zone on Route 53] and link:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-creating.html[create a record] before you can update it. The same hostname would resolve to new machine's IP address:
+
----
nslookup ip-172-31-26-200.us-west-2.compute.internal
...
Address: 172.31.23.44
----
. On the new machine, start all local services:
+
Start all local services with:
+
----
gadmin start all --local
----
The cluster node replacement is now complete.